{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up spark"
      ],
      "metadata": {
        "id": "eqxYfbhVn0ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"data processing experiment\") \\\n",
        "       .getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "raFzcm7w7O-F",
        "outputId": "7176c656-ebd2-448e-f111-810dbaa3225e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [872 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,125 kB]\n",
            "Fetched 4,606 kB in 5s (845 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=1387c90d757c7d22913cc71c13f48139814e971f4d7a7d4d9a0ff47054dd4d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "seed = 4"
      ],
      "metadata": {
        "id": "0qX1E6Ym9_41"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The example\n",
        "\n",
        "Taken from the pyspark code, creates a simple dataframe from a range and then samples it with specified fractions. Then its a simple query to count the rows grouped by key.\n",
        "\n",
        "The actual outputs don't match the expected outputs (as documented in the code, or as you might think from maths).\n",
        "\n",
        "In the first case the data only has 99 rows. Each key (0,1,2) occurs 33 times.\n",
        "\n",
        "So\n",
        "- for a fraction of 0.1 we'd expect 3.\n",
        "- for a fraction of 0.2 we'd expect 6.\n"
      ],
      "metadata": {
        "id": "8nNvHWv_oVD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raw data"
      ],
      "metadata": {
        "id": "lp0yLSzBdsXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSKFhnGY6M-b",
        "outputId": "4c4bd273-9dbc-407f-b7b7-091121046a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|   33|\n",
            "|  1|   33|\n",
            "|  2|   33|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# from https://spark.apache.org/docs/3.3.2/api/python/_modules/pyspark/sql/dataframe.html#DataFrame.sample\n",
        "\n",
        "# create a dataset using a range - each row is the modulus of 3 - so we end up with 100 rows containing keys 0, 1, or 2\n",
        "dataset = spark.range(0, 99).select((col(\"id\") % 3).alias(\"key\"))\n",
        "# and summarise the data - counts by key\n",
        "dataset.groupBy(\"key\").count().orderBy(\"key\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample with seed=4"
      ],
      "metadata": {
        "id": "d1LHjKx-dwbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now sample the data using defined fractions for each key\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=4)\n",
        "# and summarise the results - counts by key\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
        "\n",
        "# Expected:\n",
        "# +---+-----+\n",
        "# |key|count|\n",
        "# +---+-----+\n",
        "# |  0|    3|\n",
        "# |  1|    6|\n",
        "# +---+-----+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrw_-83EQ4h",
        "outputId": "07ed1ffe-b68b-47d9-83a3-8afcef1f8b82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|    2|\n",
            "|  1|    7|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample with seed=8"
      ],
      "metadata": {
        "id": "sAu8BnFHgoO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=8)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqccf8LYcp0g",
        "outputId": "d9289d50-6ae3-46f9-af42-de92e52e9ce6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|    8|\n",
            "|  1|    4|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample with seed=12"
      ],
      "metadata": {
        "id": "zWsJ-IjRgp1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=12)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4RRRi9wcpU1",
        "outputId": "abff236c-0ac8-4466-f915-3f9fdd4824fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|    4|\n",
            "|  1|    8|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This isn't what I expected - quite a bit of variability and not so accurate proportions - so lets increase the data size to see if it gets closer..."
      ],
      "metadata": {
        "id": "2GxvYLN1EXMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the size of the data (to 1000) gets a closer result..."
      ],
      "metadata": {
        "id": "mMqm5iVCqbBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 1000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=seed)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-NrM5Z5-GBb",
        "outputId": "ea534fe2-3b5a-481f-ceca-3cad3ff7ca57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|   38|\n",
            "|  1|   87|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the size of the data (to 10,000) gets closer still..."
      ],
      "metadata": {
        "id": "xz5fO6ZJqpJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 10000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=seed)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu9LEyHJ-JIX",
        "outputId": "b561fa15-4e02-4801-860f-82f9c3d2bdf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|  336|\n",
            "|  1|  697|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With 100,000 rows it's pretty much there..."
      ],
      "metadata": {
        "id": "men6OyXYqyZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 100000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=seed)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXI20ec3-MVO",
        "outputId": "68844956-7576-472c-a8ba-af931c7eb930"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0| 3314|\n",
            "|  1| 6750|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with 1,000,000 it's spot on:"
      ],
      "metadata": {
        "id": "EVBPCT-Pq3oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 1000000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=seed)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae9wdbvi-Pxn",
        "outputId": "299710ed-8574-490b-a755-76123819e066"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|33312|\n",
            "|  1|66800|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And using different seeds gives similar results this time"
      ],
      "metadata": {
        "id": "ygRtWeNKiaAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 1000000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=8)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIudS6rJie5f",
        "outputId": "b1b501a2-90a2-4600-9d8f-81a3768cc63c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|33134|\n",
            "|  1|67107|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.range(0, 1000000).select((col(\"id\") % 3).alias(\"key\"))\n",
        "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=12)\n",
        "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2fFFGYai2W1",
        "outputId": "37242dec-6ea3-4e1d-a69f-2be1d8511690"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|count|\n",
            "+---+-----+\n",
            "|  0|33294|\n",
            "|  1|66564|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as we increase the data size the proportions become more accurate - I guess nothing surprising there. Using different seeds also results in significantly different outputs when the data is small.\n",
        "\n",
        "In the next episode I'll look at sampling using multiple columns..."
      ],
      "metadata": {
        "id": "OQ5tk0M9rcUk"
      }
    }
  ]
}