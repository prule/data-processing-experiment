# -*- coding: utf-8 -*-
"""Data Processing Experiment - part 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ayTPntD0Uv591DqXYV-N96Ll0N0BIElR

# Install python dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
!python -V

# Set up dependencies
!pip install pyjson5
!pip install pyjson

# dynamically load changes to code
# %load_ext autoreload
# %autoreload 2

base_path = '/content/experiment_module'

# update the path so the custom module can be loaded
import sys
sys.path.insert(1, base_path)

# Set up custom module
# see https://saturncloud.io/blog/how-to-import-custom-modules-in-google-colab/
!git clone https://github.com/prule/data-processing-experiment-python.git experiment_module

"""## Pull latest code if required"""

!cd experiment_module && git pull

"""
# Initialize spark"""

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
!pip install pyspark
!pip install py4j

import os
import sys
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"


import findspark
findspark.init()
findspark.find()

import pyspark

from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F

spark = SparkSession \
       .builder \
       .appName("data processing experiment") \
       .getOrCreate()

"""### Load table configuration
Load the JSON5 table configuration so we can work with it.
"""

from experiment_module.src.core_prule.JsonRepository import JsonRepository
from experiment_module.src.core_prule.Configuration import Sources
import json

json_repo = JsonRepository()
table_config = json_repo.load_file('/content/experiment_module/config/sample1/sample1.tables.json5')

print(json_repo.print(table_config))

sources = Sources.from_dict(table_config)

"""# Example use
This code replicates the reference app and shows how we can use the code from the custom module to load data from configuration.

"""

from src.core_prule.Context import Context
from src.core_prule.DataFrameBuilder import DataFrameBuilder


class App:

    def go(self):
        sources = Sources.from_dict(JsonRepository().load_file(base_path + '/config/sample1/sample1.tables.json5'))

        with (SparkSession.builder.appName("Data Processing Experiment").master("local").getOrCreate()) as spark:
            context = Context(sources)

            for source in sources.sources:
                builder = DataFrameBuilder(source, base_path + "/data/", spark)

                # ------------
                # RAW
                # ------------

                # get the raw version of the dataset, everything is a string, and all columns are included
                raw = builder.raw()
                self.display("raw", raw)

                # ------------
                # SELECTED
                # ------------
                #
                # Get the selected version of the dataset, everything is a string,
                # and only configured columns are included.
                # Values will be trimmed if specified, and columns will be aliased.
                selected = builder.selected()
                self.display("selected", selected)

                # ------------
                # TYPED
                # ------------
                #
                # get the typed version of the dataset, with columns and types specified in config
                typed = builder.typed()
                self.display("typed", typed)

                # Add to context
                context.put(source.key, typed)

    def display(self, name: str, df: DataFrame):
        print()
        print(name)
        print()

        df.printSchema()
        df.sort(df.columns[0]).show(100, 10)
        print(f"Row count = {df.count()}")


app = App()
app.go()